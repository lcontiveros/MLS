{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal y descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a explorar los aspectos más típicos de la regresión lineal, al mismo tiempo que desarrollamos los\n",
    "algortimos de aprendizaje de regresión lineal por descenso de gradiente por lotes, descenso de gradiente estocástico y \n",
    "la solución por ecuación normal. \n",
    "\n",
    "Vamos a utilizar bases de datos simples y públicas que ilustren su aplicación, ventajas y desventajas. Algunos de los \n",
    "problemas habrá que programarlos directamente en la libreta, pero otros habrá que modificar modulos de python \n",
    "(archivos con extensión .py) en un editor externo (el de tu preferencia).\n",
    "\n",
    "Empezamos por el principio, inicializando las variables del entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Un ejemplo en una sola dimensión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una función muy importante para poder realizar aprendizaje máquina es la capacidad de poder manejar, cargar y gurdar datos. en esta libreta vamos a empezar con lo más básico: leer datos desde un archivo texto (o un archivo.cvs). Más adelante revisaremos como recoectar datos de internet, de archivos tipo excel o de bases de datos.\n",
    "\n",
    "*Numpy* cuenta con varios métodos para leer y guardar datos. La más utilizada para cargar datos provenientes de un archivo de texto es `loadtxt`. Para obtener la documentación de la función, simplemente ejecuta la celda siguiente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.loadtxt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante ver que esta función directamente carga los datos existentes en el archivo en un `ndarray`. ¿Y si tenemos uno o varios `ndarrays` con las cosas que hemos desarrollado y los queremos guardar en disco (por ejemplo el vector $\\omega$ de parámetros)? \n",
    "\n",
    "Vamos a abrir y a visualizar unos datos que se encuentran en el archivo `carretas.txt` (abrelos con un editor de texto si quieres ver el archivo original). En este archivo se tiene las ganancias anuales (en dolares) de unos tacos de carreta (bueno, su equivalente gringo) respecto al tamaño de la ciudad donde se encuentra la carreta. Estos datos provienen de el curso de *Machine learning* de *coursera* de *Andrew Ng*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x8b652d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEeCAYAAAD1kXAHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4e98c30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lee los datos en un nd array llamado datos\n",
    "datos = np.loadtxt('carretas.txt', comments='%', delimiter=',')\n",
    "\n",
    "# Separa los datos de entrada de los de salida.\n",
    "# si decimos que x = datos[:,0], pues se toma solo una columna de datos,\n",
    "# por lo que x sería un ndarray de forma (shape) (96,). Al decir x = datos[:, 0:1] \n",
    "# significa que vamos a tomar todas las columnas de 0 a una antes de 1, por lo\n",
    "# que x tiene una forma (96, 1). Para nuestros intereses, es mejor manejar x xomo una matriz\n",
    "# de una sola columna que como un vector de una dimensión (igual para y).\n",
    "x, y = datos[:,0:1], datos[:,1:] \n",
    "\n",
    "# T es el número de instancias y n el de atributos\n",
    "T, n = x.shape\n",
    "\n",
    "plt.plot(x, y, 'rx')\n",
    "plt.title(u'Ganancias anuales de una carreta de acuerso al tamaño de una ciudad')\n",
    "plt.xlabel(r\"Poblaci$\\'o$n ($\\times 10^4$ habitantes)\")\n",
    "plt.ylabel(r'Beneficios ($\\times 10^4$ dolares)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo, ya temos los datos. La hipótesis que hacemos es que el valor de salida lo podemos estimar como\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = h_\\omega(x^{(i)}) = \\omega_0 + \\omega_1 x^{(i)}\n",
    "$$\n",
    "\n",
    "por lo que, para poder hacer el aprendizaje en forma eficiente, es necesario ajustar la matriz de datos de entrada (en este caso con una sola columna) agregandole una columna de puros unos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.c_[np.ones_like(x), x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, el objetivo del algoritmo de mínimos cuadrados es el de minimizar el costo definido como\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2T} \\sum_{i = 1}^T (y^{(i)} - h_\\omega(x^{(i)}))^2.\n",
    "$$\n",
    "\n",
    "Por lo tanto, para saber si estamos minimizando o no, debemos ser capaces de medir la función de costo. \n",
    "\n",
    "**Desarrolla la función de costo tal como se pide abajo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def costo(x, y, w):\n",
    "    \"\"\"\n",
    "    Calcula el costo de acuerdo al criterio de MSE (mean square error) asumiendo un conjunto de datos\n",
    "    x, con una salida y, y una hipótesis lineal parametrizada por omega\n",
    "    \n",
    "    @param x: Un ndarray de dimension (T, n + 1)\n",
    "    @param y: Un ndarray de dimensión (T, 1)\n",
    "    @param w: Un ndarray de dimensión (n + 1, 1)\n",
    "    \n",
    "    @return: Un número real con el costo\n",
    "    \"\"\"\n",
    "    T, n = x.shape[0], x.shape[1] - 1\n",
    "        \n",
    "    # Puedes hacerlo en forma de ciclos\n",
    "    J = 0\n",
    "    for instancia in range(T):\n",
    "       J += (1/(2*T))*np.square(y[instancia,0]-x[instancia,0]*w[0])\n",
    "    return J\n",
    "    \n",
    "#     Puedes hacerlo directamente en forma matricial \n",
    "#     error = (1/(2*T))*(np.sum(np.square(y-x.dot(w))))\n",
    "#     return error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y para probar si está bien el programa, si calculamos $J(\\omega)$ para $\\omega = (0, 0)^T$ debe de dar (para este conjunto de datos) **32.07**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 32.07273388]\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros([n + 1, 1])\n",
    "print (costo(x, y, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, ya podemos calcular el costo. Vamos entonces a utilizar la función que acabamos de hacer para ver como funciona esto del costo para diferentes valores de $\\omega$ y ver esa famosa superficie convexa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definimos una función que depende solo de theta0 y theta1\n",
    "def costo_w(w0, w1):\n",
    "    return costo(x, y, np.array([[w0], [w1]]))\n",
    "\n",
    "# Y ahora la convertimos en una función tipo numpy (aplica para cualquier entrada de ndarrays)\n",
    "costo_vect = np.frompyfunc(costo_w, 2, 1)\n",
    "\n",
    "#Ahora generamos la lista de valores para graficar\n",
    "w0 = np.linspace(-10, 10, 100);\n",
    "w1 = np.linspace(-1, 4, 100);\n",
    "\n",
    "# Y los convertimos en matrices utilizando la función meshgrid\n",
    "w0, w1 = np.meshgrid(w0, w1)\n",
    "\n",
    "# Y calculamos los costos para cada par de theta0 y theta 1 con nuestra nueva funcion de costos vectorizada\n",
    "J = costo_vect(w0, w1)\n",
    "\n",
    "# Y graficamos el contorno\n",
    "plt.contour(w0, w1, J, 80, linewidths=0.5, colors='k')\n",
    "plt.contourf(w0, w1, J, 80, cmap=plt.cm.rainbow, vmax=J.max(), vmin=J.min())\n",
    "plt.colorbar()\n",
    "plt.xlabel(r\"$\\omega_0$\")\n",
    "plt.ylabel(r\"$\\omega_1$\")\n",
    "plt.title(r\"Funcion de costo $J(\\omega)$\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, ya tenemos todo para hacer nuestra función para encontrar la $\\omega$ optima (que como se puede ver en la superficie debería de estar por donde $\\omega_0$ vale entre 0 y -5 y $\\omega_1$ entre 1 y 2). \n",
    "\n",
    "**Desarrolla la función con descenso de gradiente.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def descenso_gradiente_lotes(x, y, w_ini, alpha, num_iter):\n",
    "    \"\"\"\n",
    "    Descenso de gradiente durante num_iter iteraciones para regresión lineal\n",
    "    \n",
    "    @param x: ndarray de dimension [T, n + 1] con los datos de entrada\n",
    "    @param y: ndarray de dimension [T, 1] con los datos de salida\n",
    "    @param w_ini: ndarray de dimension [n + 1, T] con los parametros iniciales\n",
    "    @param alpha: flotante con tamaño de paso o tasa de aprendizaje.\n",
    "    @param num_iter: numero de iteraciones (entero)\n",
    "    \n",
    "    @return: w, costo_iter donde w es un ndarray de la dimansión de w_ini con la w final, \n",
    "             mientras que costo_hist es un ndarray de dimensión [num_iter, 1] con el costo en cada iteración.\n",
    "    \n",
    "    \"\"\"\n",
    "    w = w_ini.copy()\n",
    "    costo_iter = np.zeros(num_iter)\n",
    "    \n",
    "    T, n = x.shape[0], x.shape[1] - 1\n",
    "    \n",
    "    for iter in xrange(num_iter):\n",
    "        \n",
    "        # Aqui igualmente se puede hacer por cada dato o en forma matricial\n",
    "        # por favor intenta hacerlo en forma matricial, qu es la forma \n",
    "        # eficiente de hacerlo\n",
    "        \n",
    "        # w += --aqui hay que poner código--\n",
    "        \n",
    "        costo_iter[iter] = costo(x, y, w)\n",
    "    return w, costo_iter\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo, ya lo tenemos, ahora podemos utilizar la ecuación normal para solucionar el problema de forma analítica (revisen por favor sus apuntes).\n",
    "\n",
    "**Desarrolla la función para obtener el valor de theta a partir de la ecuación normal.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ecuacion_normal(x, y):\n",
    "    \"\"\"\n",
    "    Encuentra la solución de mínimos cuadrados a partir de la ecuación normal\n",
    "\n",
    "    @param x: ndarray de dimension [T, n + 1] con los datos de entrada\n",
    "    @param y: ndarray de dimension [T, 1] con los datos de salida\n",
    "    \n",
    "    @return w: ndarray de dimension [n + 1, T] con los parametros encontrados\n",
    "    \"\"\"\n",
    "    \n",
    "    # Esta función en realidad es una sola linea de código\n",
    "    # return --inserta aqui tu código--\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para verificar las soluciones de las funciones anteriores, pues esperariamos que ante el mismo problema tuvieramos soluciones similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_ini = np.zeros((n + 1, 1))\n",
    "iteraciones = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "w_dg, costos = descenso_gradiente_lotes(x, y, w_ini, alpha, iteraciones)\n",
    "theta_n = ecuacion_normal(x, y)\n",
    "\n",
    "print \"w con descenso de gradiente: \"\n",
    "print w_dg\n",
    "\n",
    "print u\"w con ecuación normal: \"\n",
    "print w_n\n",
    "\n",
    "plt.plot(costos, 'b')\n",
    "plt.title(u'Costo por iteración')\n",
    "plt.xlabel(u'iteración')\n",
    "plt.ylabel('costo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si las soluciones no te parecen similares, pues vuelve a ejecutar la celda anterios pero con 10 veces más iteraciones. Ahora vamos a estimar los valores para una ciudad con 40000 habitantes (algo como Magdalena de Kino me imagino) y otra de 240000 (como Obregon me imagino).\n",
    "\n",
    "**Completa los pasos para realizar la estimación.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_estimar = np.array([4,24]).reshape(-1,1)\n",
    "\n",
    "#\n",
    "# Agrega el codigo necesario\n",
    "#\n",
    "\n",
    "print \"Los valores estimados con w_dg son: \"\n",
    "print y_estimado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si los valores que obtuviste son cercanos a 1 (10000 dolares) y 24.3 (243000 dolares) entonces estamos en los valores esperados. Ahora vamos a usar estos valores para graficar los datos reales y la estimación realizada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x[:,1], y, 'xr')\n",
    "plt.plot(x_estimar[:,0], y_estimado, '-b')\n",
    "plt.title(u'Ganancias anuales de una carreta de acuerso al tamaño de una ciudad')\n",
    "plt.xlabel(r\"Poblaci$\\'o$n ($\\times 10^4$ habitantes)\")\n",
    "plt.ylabel(r'Beneficios ($\\times 10^4$ dolares)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Felicidades** Acabas de terminar el primer algoritmo de aprendizaje (y el más usado en el mundo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Un ejemplo en multiples dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los algortimos que se realizaron ya funcionan muy bien para muchas dimensiones, pues no se espera tener mucho problema para utilizarlos. Así que ahora vamos a cargar los datos y vamos a graficar la salida respecto a cada una de las dos variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos = np.loadtxt('casas_portland.txt', comments='%', delimiter=',')\n",
    "x, y = datos[:, :-1], datos[:,-1:] \n",
    "\n",
    "# T es el número de instancias y n el de atributos\n",
    "T, n = x.shape\n",
    "\n",
    "plt.plot(x[:,0], y, 'rx')\n",
    "plt.title(u'Costo de una casa en relación a su tamaño')\n",
    "plt.xlabel(u\"tamaño (pies cuadrados)\")\n",
    "plt.ylabel('costo ')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x[:,1], y, 'rx')\n",
    "plt.title(u'Costo de una casa en relación al número de cuartos')\n",
    "plt.xlabel(\"cuartos\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de realizar el aprendizaje podemos ver que mientras una de las variables se mide en miles de pies cuadrados, la otra variable tiene valores de 1 a 4. Esto es un problema para el algoritmo del descenso de gradiente, por lo que es necesario normalizar los datos (solo para este algoritmo) y que funcione de manera correcta. \n",
    "\n",
    "Para normalizar requerimos de dos pasos, por un lado, obtener los valores de medias y desviaciones estandares por atributo, y en segundo lugar, realizar la normalización. Los valores de medias y desviaciones estandares hay que guardarlos, ya que serán necesarios para poder normalizar los datos que se quiera estimar.\n",
    "\n",
    "**Escribe la función que devuelve los valores de medias t desviaciones estandares.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def obtiene_medias_desviaciones(x):\n",
    "    \"\"\"\n",
    "    Obtiene las medias y las desviaciones estandar atributo a atributo.\n",
    "    \n",
    "    @param x: un ndarray de dimensión (T, n) donde T es el númro de elementos y n el número de atributos\n",
    "    \n",
    "    @return: medias, desviaciones donde ambos son ndarrays de dimensiones (n,) con las medias y las desviaciones \n",
    "             estandar respectivamente.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Escribe aqui el código\n",
    "    #\n",
    "    #\n",
    "    #    \n",
    "    return medias, desviaciones\n",
    "\n",
    "def normaliza(x, medias, desviaciones):\n",
    "    \"\"\"\n",
    "    Normaliza los datos x\n",
    "\n",
    "    @param x: un ndarray de dimensión (T, n) donde T es el númro de elementos y n el número de atributos\n",
    "    @param medias: ndarray de dimensiones (n,) con las medias con las que se normalizará\n",
    "    @param desviaciones: ndarray de dimensiones (n,) con las desviaciones con las que se normalizará\n",
    "    \n",
    "    @return: x_norm un ndarray de las mismas dimensiones de x pero normalizado\n",
    "    \n",
    "    \"\"\"\n",
    "    return (x - medias) / desviaciones\n",
    "        \n",
    "\n",
    "# Y ahora vamos a hacer algo muy simple para probar, que pueden corroborar con el uso de una calculadora común.\n",
    "x_prueba = np.array([[1, 300],\n",
    "                    [3, 100],\n",
    "                    [2, 400],\n",
    "                    [4, 200]])\n",
    "m, d = obtiene_medias_desviaciones(x_prueba)\n",
    "\n",
    "print \"Los datos son: \"\n",
    "print x_prueba\n",
    "print \"Las medias son: \"\n",
    "print m\n",
    "print \"Las desviaciones son: \"\n",
    "print d\n",
    "print \"Los datos normalizados son: \"\n",
    "print normaliza(x_prueba, m, d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo, entonces ya podemos hacer descenso de gradiente, o casi. El problema es que no sabemos cual sería el mejor valor para $\\omega$. Escoge el valor de $\\omega$ realizando una gráfica de 50 iteraciones solamente para valores desde 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, ... y decide cual de los valores es el que más te conviene.\n",
    "\n",
    "**Selecciona un valor, especifica aquí cual es, y justifica porque lo seleccionaste.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medias, desviaciones = obtiene_medias_desviaciones(x)\n",
    "x_norm = np.c_[np.ones((T, 1)), normaliza(x, medias, desviaciones)]\n",
    "\n",
    "w_ini = np.zeros((n + 1, 1))\n",
    "num_iters = 50\n",
    "\n",
    "alpha = 0.0001  # Aqui es donde hay que hacer las pruebas\n",
    "\n",
    "w, costos_iters = descenso_gradiente_lotes(x_norm, y, w_ini, alpha, num_iters)\n",
    "print w\n",
    "\n",
    "plt.plot(costos_iters, '-b')\n",
    "plt.title(r\"La curva de aprendizaje para $\\alpha =$ \" + str(alpha))\n",
    "plt.xlabel('iteraciones')\n",
    "plt.ylabel('costo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilizando todo el número de iteraciones necesarias, encuentra el valor de $\\omega$ utilizando el descenso de gradiente.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aqui ya no pongo código, esto debe ser relativamente simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora para comparar, vamos a realizar el aprendizaje con la ecuacion normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_n = ecuacion_normal(np.c_[np.ones((T, 1)), x], y)\n",
    "print w_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obten el valor de una casa de 1650 pies cuadrados y 3 recamaras con las thetas obtenidas con ambos algoritmos y verifica que es el mismo resultado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Escribe aquí el código\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
